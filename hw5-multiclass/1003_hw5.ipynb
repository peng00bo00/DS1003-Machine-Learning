{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Machine Learning Homework 5<br/>\n",
    "Author: Binqian Zeng<br/>\n",
    "NetId: bz866<br/>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If $f_{1}(x),...,f_{m}(x) : R^{n} \\rightarrow R$ are convex, then their pointwise maximum\n",
    "$$f(x) = max\\{f_{1}(x),...,f_{m}(x)\\}$$ is also convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**2. Convex Surrogate Loss Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**2.1 Hinge loss is a convex surrogate for 0/1 loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.1.1.(a)<br/>\n",
    "If $y=sign(f(x))$, $1(y\\neq sign(f(x))=0$. Obviously,\n",
    "$$1(y\\neq sign(f(x)) \\leq max\\{0,1-yf(x)\\}$$\n",
    "\n",
    "If $y\\neq sign(f(x))$, $1(y\\neq sign(f(x))=1$, and $yf(x) \\leq 0$, $1-yf(x)\\geq 1$. Obviously, \n",
    "$$1(y\\neq sign(f(x)) \\leq max\\{0,1-yf(x)\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.1.1.(b)<br/>\n",
    "Based on the result given in 1.1, both $0$ and $1-m$ are convex function of the margin $m$. Then we can say that $max\\{0,1-m\\}$ is a convex function of the margin $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.1.1.(c)<br/>\n",
    "A function $f:R^{n}\\rightarrow R^{m}$ is an affine function and an affine function is convex and concave. Then $1-yw^{T}x$ is a convex function. $max\\{0,1-yw^{T}x\\}$ is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**2.2 Generalized Hinge Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2.1<br/>\n",
    "set $y*\\in Y$, $h(x,y*)\\geq h(x,y)$ for all $y\\in Y$<br/>\n",
    "Since $$f(x) = argmax_{y\\in Y}h(x,y)$$\n",
    "Then $$y* = f(x)$$\n",
    "So $$h(x,y) \\leq h(x,f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2.2<br/>\n",
    "Based on the result shown in 2.1, we have \n",
    "$$h(x,y) \\leq h(x,f(x))$$\n",
    "so\n",
    "$$h(x,f(x)) - h(x,y)\\geq0$$\n",
    "Then we have \n",
    "$$\\Delta(y,f(x))\\leq \\Delta(y,f(x)) + h(x,f(x)) - h(x,y)$$\n",
    "and Obviously\n",
    "$$\\Delta(y,f(x)) + h(x,f(x)) - h(x,y) \\leq max_{y'\\in Y}[\\Delta(y,y')+h(x,y')-h(x,y)]$$\n",
    "So \n",
    "$$\\Delta(y,f(x))\\leq max_{y'\\in Y}[\\Delta(y,y') + h(x,y')-h(x,y)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2.3<br/>\n",
    "Based on the fomula shown in 2.2\n",
    "$$l(h,(x,y))=max_{y'\\in Y}[\\Delta(y,y')+h(x,y')-h(x,y)]$$\n",
    "we have \n",
    "$$l(h_{w},(x_{i},y_{i})) = max_{y\\in Y}[\\Delta(y_{i},y) + h_{w}(x_{i},y) - h_{w}(x_{i},y_{i})]$$\n",
    "for \n",
    "$$H = \\{h_{w}(x,y) = <w,\\Phi(x,y)> | w\\in R^{d}\\}$$\n",
    "we have \n",
    "$$l(h_{w},(x_{i},y_{i})) = max_{y\\in Y}[\\Delta(y_{i},y) + <w,\\Phi(x_{i},y)>-<w,\\Phi(x_{i},y_{i})>]$$\n",
    "For the Algebraic properties of vectors\n",
    "$$l(h_{w},(x_{i},y_{i}) = max_{y\\in Y}[\\Delta(y_{i},y)+ <w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2.4<br/>\n",
    "2.2.4(a)<br/>\n",
    "$$<w,\\Phi(x_{i}-y) - \\Phi(x_{i},y_{i})>$$\n",
    "$$= <w,\\Phi(x_{i}-y)>-<w,\\Phi(x_{i},y_{i})>$$\n",
    "Since $w\\in R^{d}$, $\\Phi:X*Y\\rightarrow R^{d}$\n",
    "$$<w,\\Phi(x_{i}-y)>$$\n",
    "$$=\\Sigma_{j}^{n}w^{(j)}\\Phi^{(j)}(x_{i}-y)$$\n",
    "which is a linear combination of $w$, then the expression \n",
    "$$\\Delta(y_{i},y) + <w, \\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>$$ is an affine function of $w$\n",
    "\n",
    "2.2.4(b)<br/>\n",
    "Since the affine function is always a convex function, we know that \n",
    "$$\\Delta(y_{i},y) + <w, \\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>$$ is a convex function. And based on the result given in 1.1\n",
    "$$max_{y\\in Y}[\\Delta(y_{i},y) + <w, \\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$ is a convex function of $w$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2.5<br/>\n",
    "Based on results obtained from previous questions, we have \n",
    "$$\\Delta(y_{i},f_{w}(x_{i}))\\leq max_{y'\\in Y}[\\Delta(y_{i},y')+h_{w}(x_{i},y')-h_{w}(x_{i},y_{i})]$$\n",
    "and \n",
    "$$l(h_{w},(x_{i},y_{i})) = max_{y'\\in Y}[\\Delta(y_{i},y')+h_{w}(x_{i},y')-h_{w}(x_{i},y_{i})]$$\n",
    "Hence we know that \n",
    "$$l(h_{w},(x_{i},y_{i})) \\geq\\Delta(y_{i},f_{w}(x_{i}))$$\n",
    "$l(h_{w},(x_{i},y_{i}))$ is one of upper bounds of $\\Delta(y_{i},f_{w}(x_{i}))$. If we can make the upper bound small, then the loss we care about will also be small. That's why we can take $l(h_{w},(x_{i},y_{i}))$ as a convex surrogate for $\\Delta(y_{i},f_{w}(x_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**3. SGD for Multiclass SVM **<br/>\n",
    "3.1.(a)<br/>\n",
    "Based on the result given in the question that \n",
    "$$w\\rightarrow max_{y\\in Y}[\\Delta(y_{i},y)+<w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$\n",
    "is a convex function\n",
    "and $$\\dfrac{1}{n}>0$$ and linear combination of convex function is a convex function<br/>\n",
    "So \n",
    "$$\\dfrac{1}{n}\\Sigma_{i=1}^{n}max_{y\\in Y}[\\Delta(y_{i},y)+<w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$\n",
    "is a convex function of $w$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.1.(b)<br/>\n",
    "Since every norm of $R^{n}$ is convex, then we know that $||w||$ is convex. Also, $x^2$ is a convex function and it is non-decreasing in the range of $[0,\\infty]$. Then we can say that $||w||^{2}$ is a convex function of $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.1.(c)<br/>\n",
    "Based on results obtained from 3.1.(a) and 3.1.(b) that both \n",
    "$$\\dfrac{1}{n}\\Sigma_{i=1}^{n}max_{y\\in Y}[\\Delta(y_{i},y)+<w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$\n",
    "and \n",
    "$$||w||^{2}$$ \n",
    "are convex function. Also $J(w)$ is a linear combination of the two previous function. So \n",
    "$$J(w) = \\lambda||w||^{2} + \\dfrac{1}{n}\\Sigma_{i=1}^{n}max_{y\\in Y}[\\Delta(y_{i},y) + <w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$ is a convex function of $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.2<br/>\n",
    "$$J(w) = \\lambda||w||^{2} + \\dfrac{1}{n}\\Sigma_{i=1}^{n}max_{y\\in Y}[\\Delta(y_{i},y) + <w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$ \n",
    "$$\\dfrac{\\partial\\lambda||w||^{2}}{\\partial w} = 2\\lambda w$$\n",
    "Also, we set $\\hat{y}_{i} = argmax_{y\\in Y}[\\Delta(y_{i},y)+<w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$\n",
    "Then \n",
    "$$\\dfrac{1}{n}\\Sigma_{i=1}^{n}max_{y\\in Y}[\\Delta(y_{i},y) + <w,\\Phi(x_{i},y)-\\Phi(x_{i},y_{i})>]$$\n",
    "$$ = \\dfrac{1}{n}\\Sigma_{i=1}^{n}[\\Delta(y_{i},\\hat{y}_{i})+<w,\\Phi(x_{i},\\hat{y}_{i}-\\Phi(x_{i},y_{i})>]$$\n",
    "Since \n",
    "$$(\\Phi(x_{i},\\hat{y}_{i})-\\Phi(x_{i},y_{i})) \\in \\partial[\\Delta(y_{i},\\hat{y}_{i})+<w,\\Phi(x_{i},\\hat{y}_{i}-\\Phi(x_{i},y_{i})>]$$\n",
    "Then we have a subgradient of $J(w)$\n",
    "$$2\\lambda w+ \\dfrac{1}{n}\\Sigma_{i=1}^{n}(\\Phi(x_{i},\\hat{y}_{i})-\\Phi(x_{i},y_{i})) \\in \\partial\\{J(w)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.3<br/>\n",
    "$$2\\lambda w + (\\Phi(x_{i},\\hat{y}_{i})-\\Phi(x_{i},y_{i})) \\in \\partial J^{(s)}(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.4<br/>\n",
    "$$2\\lambda w + \\dfrac{1}{m}\\Sigma_{j=0}^{m-1}(\\Phi(x_{i+j},\\hat{y}_{i+j})-\\Phi(x_{i+j},y_{i+j}))\\in \\partial J^{(m)}(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[Optional] 4. Another Formulation of Generalized Hinge Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4.1<br/>\n",
    "Based on the result obtained in 2.2.2, we have \n",
    "$$l(h,(x,y)) =max_{y'\\in Y} [\\Delta(y,y')+h(x,y')-h(x,y)]$$\n",
    "when $y=y'$\n",
    "$$m_{i,y'}(h) = h(x_{i},y_{i})-h(x_{i},y')$$\n",
    "and put in the previous equationm, we have \n",
    "$$l(h,(x_{i},y_{i})) = max_{y'\\in Y}[\\Delta(y_{i},y')-m_{i,y'}(h)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4.2<br/>\n",
    "Based on the result obtained in 2.2.2, we have \n",
    "$$\\Delta(y,f(x))\\leq max_{y'\\in Y}[\\Delta(y,y') + h(x,y')-h(x,y)]$$\n",
    "and the question also supposes that \n",
    "$$\\Delta(y,y')\\geq 0 for all y,y' \\in Y$$\n",
    "then \n",
    "$$\\Delta(y_{i},y)-m_{i,y}(h) \\geq 0$$\n",
    "So we have \n",
    "$$[\\Delta(y_{i},y)-m_{i,y}(h)]_{+} = \\Delta(y_{i},y)-m_{i,y}(h)$$\n",
    "$$max_{y\\in Y}([\\Delta(y_{i},y)-m_{i,y}(h)]_{+}) = max_{y\\in Y}( \\Delta(y_{i},y)-m_{i,y}(h))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4.3<br/>\n",
    "Based on the assumption given from the question, \n",
    "$$l(h,(x_{i},y_{i})) = \\Delta(y_{i},y) - m_{i,y}(h) $$\n",
    "and \n",
    "$$ m_{i,y}(h) = h(x_{i},y_{i})-h(x_{i},y) \\geq \\Delta(y_{i},y)$$\n",
    "If $\\Delta(y,y) = 0$, we must have $m_{i,y}(h) = h(x_{i},y_{i})-h(x_{i},y)=0$\n",
    "So we have \n",
    "$$\n",
    "l(h,(x_{i},y_{i}))=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[Optional] 5. Hinge Loss is a Special Case of Generalized Hinge Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the assumption given for $h$\n",
    "If $y = \\hat{y}$, Obviously, we always have \n",
    "$$l(h,(x,y)) = 0$$\n",
    "If $y = 1$\n",
    "$$l(h,(x,1)) = max\\{\\Delta(1,1)+h(x,1)-h(x,1), \\Delta(1,-1)+h(x,-1)-h(x,1)\\}$$\n",
    "$$ = max\\{0, 1-1*g(x)\\} = max\\{0,1-yg(x)\\}$$\n",
    "If $y = -1$\n",
    "$$l(h,(x,-1)) = max\\{\\Delta(-1,1)+h(x,-1)-h(x,1), \\Delta(-1,-1)+h(x,-1)-h(x,-1)\\}$$\n",
    "$$ = max\\{0, 1+(-1)*g(x)\\} = max\\{0,1-yg(x)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Multiclass Classification - Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 One-vs-All(also known as One-vs-Rest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x110900160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnXMug7y4O+dcBnlxd865DPofqFNp/P5s+gsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1108842b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the  training data\n",
    "np.random.seed(2)\n",
    "X, y = make_blobs(n_samples=300,cluster_std=.25, centers=np.array([(-3,1),(0,2),(3,1)]))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1,\n",
       "       2, 1, 1, 2, 1, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 2, 0, 0,\n",
       "       1, 2, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1,\n",
       "       2, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2, 2, 1, 2, 0, 1, 1,\n",
       "       0, 1, 1, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 2, 2, 0, 2, 0, 2, 1, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2,\n",
       "       0, 2, 1, 1, 0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0,\n",
       "       2, 2, 2, 0, 2, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 0, 1, 1, 2,\n",
       "       1, 2, 2, 1, 1, 2, 1, 2, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2, 0, 1, 0, 2, 0, 2, 1, 0,\n",
       "       0, 0, 2, 0, 1, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 2, 0, 2, 2, 2,\n",
       "       2, 2, 0, 1, 2, 2, 0, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 0, 2, 0, 2, 0, 1,\n",
       "       0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import features as f\n",
    "import numpy as np\n",
    "from sklearn import pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "class OneVsAllClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    One-vs-all classifier\n",
    "    We assume that the classes will be the integers 0,..,(n_classes-1).\n",
    "    We assume that the estimator provided to the class, after fitting, has a \"decision_function\" that \n",
    "    returns the score for the positive class.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_classes):      \n",
    "        \"\"\"\n",
    "        Constructed with the number of classes and an estimator (e.g. an\n",
    "        SVM estimator from sklearn)\n",
    "        @param estimator : binary base classifier used\n",
    "        @param n_classes : number of classes\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes \n",
    "        self.estimators = [clone(estimator) for _ in range(n_classes)]\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This should fit one classifier for each class.\n",
    "        self.estimators[i] should be fit on class i vs rest\n",
    "        @param X: array-like, shape = [n_samples,n_features], input data\n",
    "        @param y: array-like, shape = [n_samples,] class labels\n",
    "        @return returns self\n",
    "        \"\"\"\n",
    "        #Your code goes here\n",
    "        self.estimators(X,y)\n",
    "        \n",
    "        self.fitted = True  \n",
    "        return self   \n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Returns the score of each input for each class. Assumes\n",
    "        that the given estimator also implements the decision_function method (which sklearn SVMs do), \n",
    "        and that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_features] input data\n",
    "        @return array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        if not hasattr(self.estimators[0], \"decision_function\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have a decision_function attribute.\")\n",
    "        \n",
    "        #Replace the following return statement with your code\n",
    "        return self.estimators.decision_function(self.transfor(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples,n_features] input data\n",
    "        @returns array-like, shape = [n_samples,] the predicted classes for each input\n",
    "        \"\"\"\n",
    "        #Replace the following return statement with your code\n",
    "        return self.estimator_.predict(self.transform(X))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5b9c8bdb329c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hinge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf_onevsall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsAllClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf_onevsall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-5d3288ce7887>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#Your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "#Here we test the OneVsAllClassifier\n",
    "from sklearn import svm\n",
    "svm_estimator = svm.LinearSVC(loss='hinge', fit_intercept=False, C=200)\n",
    "clf_onevsall = OneVsAllClassifier(svm_estimator, n_classes=3)\n",
    "clf_onevsall.fit(X,y)\n",
    "\n",
    "for i in range(3) :\n",
    "    print(\"Coeffs %d\"%i)\n",
    "    print(clf_onevsall.estimators[i].coef_) #Will fail if you haven't implemented fit yet\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = min(X[:,0])-3,max(X[:,0])+3\n",
    "y_min, y_max = min(X[:,1])-3,max(X[:,1])+3\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = clf_onevsall.predict(mesh_input)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y, clf_onevsall.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    #your code goes here, and replaces following return\n",
    "    return 0\n",
    "\n",
    "def sgd(X, y, num_outFeatures, subgd, eta = 0.1, T = 10000):\n",
    "    '''\n",
    "    Runs subgradient descent, and outputs resulting parameter vector.\n",
    "    @param X: array-like, shape = [n_samples,n_features], input training data \n",
    "    @param y: array-like, shape = [n_samples,], class labels\n",
    "    @param num_outFeatures: number of class-sensitive features\n",
    "    @param subgd: function taking x,y and giving subgradient of objective\n",
    "    @param eta: learning rate for SGD\n",
    "    @param T: maximum number of iterations\n",
    "    @return: vector of weights\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    #your code goes here and replaces following return statement\n",
    "    return 0\n",
    "\n",
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=3, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,x,y,w):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        #Your code goes here and replaces the following return statement\n",
    "        return 0\n",
    "        \n",
    "    def fit(self,X,y,eta=0.1,T=10000):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = sgd(X,y,self.num_outFeatures,self.subgradient,eta,T)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        #Your code goes here and replaces following return statement\n",
    "        return 0\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "\n",
    "        #Your code goes here and replaces following return statement\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the following code tests the MulticlassSVM and sgd\n",
    "#will fail if MulticlassSVM is not implemented yet\n",
    "est = MulticlassSVM(6,lam=1)\n",
    "est.fit(X,y)\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "Z = est.predict(mesh_input)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y, est.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
