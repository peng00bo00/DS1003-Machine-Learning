{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decision_Tree(BaseEstimator):\n",
    "     \n",
    "    def __init__(self, split_loss_function, leaf_value_estimator,\n",
    "                 depth=0, min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize the decision tree classifier\n",
    "\n",
    "        :param split_loss_function: method for splitting node\n",
    "        :param leaf_value_estimator: method for estimating leaf value\n",
    "        :param depth: depth indicator, default value is 0, representing root node\n",
    "        :param min_sample: an internal node can be splitted only if it contains points more than min_smaple\n",
    "        :param max_depth: restriction of tree depth.\n",
    "        '''\n",
    "        self.split_loss_function = split_loss_function\n",
    "        self.leaf_value_estimator = leaf_value_estimator\n",
    "        self.depth = depth\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        This should fit the tree classifier. \n",
    "        :param X: a numpy array of training data, shape = (n, m)\n",
    "        :param y: a numpy array of labels, shape = (n, 1)\n",
    "\n",
    "        :return self\n",
    "        '''\n",
    "\n",
    "        self.split_id = -1\n",
    "        if len(X) > self.min_sample and self.depth < self.max_depth:\n",
    "            # Attempt to split this node\n",
    "            self.split_value = np.inf\n",
    "            min_loss = self.split_loss_function(y)\n",
    "\n",
    "            for attr_id in range(X.shape[1]):\n",
    "                attr_value = sorted(list(set(X[:, attr_id])))\n",
    "                for i in range(len(attr_value) - 1):\n",
    "                    v = (attr_value[i] + attr_value[i + 1]) / 2. #compute midpoint\n",
    "\n",
    "                    left_index_temp = np.where(X[:, attr_id] <= v)[0]\n",
    "                    right_index_temp = np.where(X[:, attr_id] > v)[0]\n",
    "\n",
    "                    left_loss = self.split_loss_function(y[left_index_temp])\n",
    "                    right_loss = self.split_loss_function(y[right_index_temp])\n",
    "\n",
    "                    split_loss = (len(left_index_temp) * left_loss + len(right_index_temp) * right_loss) / float(len(y))\n",
    "\n",
    "                    if split_loss < min_loss: # if loss decreases, update split_id and split_value \n",
    "                        self.split_id = attr_id\n",
    "                        self.split_value = v\n",
    "                        min_loss = split_loss\n",
    "\n",
    "        if self.split_id == -1:\n",
    "            self.is_leaf = True\n",
    "            self.value = self.leaf_value_estimator(y)\n",
    "            return self\n",
    "\n",
    "        # Create subtree on left and right\n",
    "        self.is_leaf = False\n",
    "\n",
    "        self.left = Decision_Tree(self.split_loss_function, self.leaf_value_estimator,\n",
    "                          depth=self.depth+1, min_sample=self.min_sample,  max_depth=self.max_depth)\n",
    "        self.right = Decision_Tree(self.split_loss_function, self.leaf_value_estimator,\n",
    "                          depth=self.depth+1, min_sample=self.min_sample,  max_depth=self.max_depth)\n",
    "        left_index = np.where(X[:, self.split_id] <= self.split_value)\n",
    "        right_index = np.where(X[:, self.split_id] > self.split_value)\n",
    "        self.left.fit(X[left_index], y[left_index])\n",
    "        self.right.fit(X[right_index], y[right_index])\n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        '''\n",
    "        Predict label by decision tree\n",
    "\n",
    "        :param instance: a numpy array with new data, shape (1, m)\n",
    "\n",
    "        :return whatever is returned by leaf_value_estimator for leaf containing instance\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        if instance[self.split_id] <= self.split_value:\n",
    "            return self.left.predict_instance(instance)\n",
    "        else:\n",
    "            return self.right.predict_instance(instance)\n",
    "\n",
    "# Regression Tree Specific Code\n",
    "def mean_absolute_deviation_around_median(y):\n",
    "    '''\n",
    "    Calulate the mean absolute deviation of given target list\n",
    "    \n",
    "    :param y: a numpy array of targets shape = (n, 1)\n",
    "    :return mae\n",
    "    '''\n",
    "    median_value = np.median(y)\n",
    "    mae = np.mean((abs(y - median_value)))\n",
    "    return mae\n",
    "\n",
    "class Regression_Tree():\n",
    "    '''\n",
    "    :attribute loss_function_dict: dictionary containing the loss functions used for splitting\n",
    "    :attribute estimator_dict: dictionary containing the estimation functions used in leaf nodes\n",
    "    '''\n",
    "\n",
    "    loss_function_dict = {\n",
    "        'mse': np.var,\n",
    "        'mae': mean_absolute_deviation_around_median\n",
    "    }\n",
    "\n",
    "    estimator_dict = {\n",
    "        'mean': np.mean,\n",
    "        'median': np.median\n",
    "    }\n",
    "    \n",
    "    def __init__(self, loss_function='mse', estimator='mean', min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize Regression_Tree\n",
    "        :param loss_function(str): loss function used for splitting internal nodes\n",
    "        :param estimator(str): value estimator of internal node\n",
    "        '''\n",
    "\n",
    "        self.tree = Decision_Tree(self.loss_function_dict[loss_function],\n",
    "                                  self.estimator_dict[estimator],\n",
    "                                  0, min_sample, max_depth)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tree.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        value = self.tree.predict_instance(instance)\n",
    "        return value\n",
    "\n",
    "# Classification Tree specific code\n",
    "\n",
    "def compute_entropy(y):\n",
    "    '''\n",
    "    Calulate the entropy of given label list\n",
    "    \n",
    "    :param y: a numpy array of labels shape = (n, 1)\n",
    "    :return entropy: entropy value\n",
    "    '''\n",
    "    label_cnt = Counter(y.reshape(len(y)))\n",
    "    prob_list = [value/float(sum(label_cnt.values())) for value in label_cnt.values()]\n",
    "    entropy = 0.\n",
    "    for prob in prob_list:\n",
    "        if prob == 0.:\n",
    "            pass\n",
    "        else:\n",
    "            entropy += -1. * prob * np.log(prob)\n",
    "    return entropy\n",
    "\n",
    "def compute_gini(y):\n",
    "    '''\n",
    "    Calulate the gini index of label list\n",
    "    \n",
    "    :param y: a numpy array of labels shape = (n, 1)\n",
    "    :return gini: gini index value\n",
    "    '''\n",
    "    label_cnt = Counter(y.reshape(len(y)))\n",
    "    prob_list = [value/float(sum(label_cnt.values())) for value in label_cnt.values()]\n",
    "    gini = 0.\n",
    "    for prob in prob_list:\n",
    "        gini += prob * (1 - prob)\n",
    "    return gini\n",
    "\n",
    "def most_common_label(y):\n",
    "    '''\n",
    "    Find most common label\n",
    "    '''\n",
    "    label_cnt = Counter(y.reshape(len(y)))\n",
    "    label = label_cnt.most_common(1)[0][0]\n",
    "    return label\n",
    "\n",
    "class Classification_Tree(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    loss_function_dict = {\n",
    "        'entropy': compute_entropy,\n",
    "        'gini': compute_gini\n",
    "    }\n",
    "\n",
    "    def __init__(self, loss_function='entropy', min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        :param loss_function(str): loss function for splitting internal node\n",
    "        '''\n",
    "\n",
    "        self.tree = Decision_Tree(self.loss_function_dict[loss_function],\n",
    "                                most_common_label,\n",
    "                                0, min_sample, max_depth)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tree.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        value = self.tree.predict_instance(instance)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = np.loadtxt('svm-train.txt')\n",
    "data_test = np.loadtxt('svm-test.txt')\n",
    "x_train, y_train = data_train[:, 0: 2], data_train[:, 2].reshape(-1, 1)\n",
    "x_test, y_test = data_test[:, 0: 2], data_test[:, 2].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change target to 0-1 label\n",
    "y_train_label = np.array(list(map(lambda x: 1 if x > 0 else 0, y_train))).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training classifiers with different depth\n",
    "clf1 = Classification_Tree(max_depth=1)\n",
    "clf1.fit(x_train, y_train_label)\n",
    "\n",
    "clf2 = Classification_Tree(max_depth=2)\n",
    "clf2.fit(x_train, y_train_label)\n",
    "\n",
    "clf3 = Classification_Tree(max_depth=3)\n",
    "clf3.fit(x_train, y_train_label)\n",
    "\n",
    "clf4 = Classification_Tree(max_depth=4)\n",
    "clf4.fit(x_train, y_train_label)\n",
    "\n",
    "clf5 = Classification_Tree(max_depth=5)\n",
    "clf5.fit(x_train, y_train_label)\n",
    "\n",
    "clf6 = Classification_Tree(max_depth=6)\n",
    "clf6.fit(x_train, y_train_label)\n",
    "\n",
    "# Plotting decision regions\n",
    "x_min, x_max = x_train[:, 0].min() - 1, x_train[:, 0].max() + 1\n",
    "y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                        [clf1, clf2, clf3, clf4, clf5, clf6],\n",
    "                        ['Depth = {}'.format(n) for n in range(1, 7)]):\n",
    "\n",
    "    Z = np.array([clf.predict_instance(x) for x in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_train[:, 0], x_train[:, 1], c=y_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decision tree with tree model in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=5)\n",
    "clf.fit(x_train, y_train_label)\n",
    "export_graphviz(clf, out_file='tree_classifier.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize decision tree\n",
    "!dot -Tpng tree_classifier.dot -o tree_classifier.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='tree_classifier.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare first splitting point\n",
    "print('Split_id of root node: {}'.format(clf2.tree.split_id))\n",
    "print('Split_value of root node: {}'.format(clf2.tree.split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_reg = Regression_Tree(max_depth=5)\n",
    "tree_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decision tree with tree model in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treg_sk = DecisionTreeRegressor(max_depth=3, min_samples_split=5, criterion='mse')\n",
    "treg_sk.fit(x_train, y_train)\n",
    "export_graphviz(treg_sk, out_file='tree_regressor.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree_regressor.dot -o tree_regressor.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='tree_regressor.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare first splitting point\n",
    "print('Split_id of root node: {}'.format(tree_reg.tree.split_id))\n",
    "print('Split_value of root node: {}'.format(tree_reg.tree.split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit regression tree to 1-dim regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_krr_train = np.loadtxt('krr-train.txt')\n",
    "data_krr_test = np.loadtxt('krr-test.txt')\n",
    "x_krr_train, y_krr_train = data_krr_train[:,0].reshape(-1,1),data_krr_train[:,1].reshape(-1,1)\n",
    "x_krr_test, y_krr_test = data_krr_test[:,0].reshape(-1,1),data_krr_test[:,1].reshape(-1,1)\n",
    "\n",
    "# Training regression trees with different depth\n",
    "clf1 = Regression_Tree(max_depth=1,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf1.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf2 = Regression_Tree(max_depth=2,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf2.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf3 = Regression_Tree(max_depth=3,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf3.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf4 = Regression_Tree(max_depth=4,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf4.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf5 = Regression_Tree(max_depth=5,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf5.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf6 = Regression_Tree(max_depth=6,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf6.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "plot_size = 0.001\n",
    "x_range = np.arange(0., 1., plot_size).reshape(-1, 1)\n",
    "\n",
    "f2, axarr2 = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                        [clf1, clf2, clf3, clf4, clf5, clf6],\n",
    "                        ['Depth = {}'.format(n) for n in range(1, 7)]):\n",
    "\n",
    "    y_range_predict = np.array([clf.predict_instance(x) for x in x_range]).reshape(-1, 1)\n",
    "  \n",
    "    axarr2[idx[0], idx[1]].plot(x_range, y_range_predict, color='r')\n",
    "    axarr2[idx[0], idx[1]].scatter(x_krr_train, y_krr_train, alpha=0.8)\n",
    "    axarr2[idx[0], idx[1]].set_title(tt)\n",
    "    axarr2[idx[0], idx[1]].set_xlim(0, 1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pseudo-residual function.\n",
    "#Here you can assume that we are using L2 loss\n",
    "\n",
    "def pseudo_residual_L2(train_target, train_predict):\n",
    "    '''\n",
    "    Compute the pseudo-residual based on current predicted value. \n",
    "    '''\n",
    "    return train_target - train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gradient_boosting():\n",
    "    '''\n",
    "    Gradient Boosting regressor class\n",
    "    :method fit: fitting model\n",
    "    '''\n",
    "    def __init__(self, n_estimator, pseudo_residual_func, learning_rate=0.1, min_sample=5, max_depth=3):\n",
    "        '''\n",
    "        Initialize gradient boosting class\n",
    "        \n",
    "        :param n_estimator: number of estimators\n",
    "        :pseudo_residual_func: function used for computing pseudo-residual\n",
    "        :param learning_rate: step size of gradient descent\n",
    "        '''\n",
    "        self.n_estimator = n_estimator\n",
    "        self.pseudo_residual_func = pseudo_residual_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''\n",
    "        Fit gradient boosting model\n",
    "        '''\n",
    "        predict_value = np.zeros((len(train_target), 1))\n",
    "        pseodo_residual = self.pseudo_residual_func(train_target, predict_value)\n",
    "        model_dict = {}\n",
    "        for i in range(self.n_estimator):\n",
    "            decision_stump = Regression_Tree(min_sample=self.min_sample, max_depth=self.max_depth)\n",
    "            decision_stump.fit(train_data, pseodo_residual)\n",
    "            model_dict[i] = decision_stump\n",
    "            predict_value += self.learning_rate * np.array([decision_stump.predict_instance(x) for x in train_data]).reshape(-1, 1)\n",
    "            pseodo_residual = self.pseudo_residual_func(train_target, predict_value)\n",
    "        self.model_dict = model_dict\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        '''\n",
    "        Predict value\n",
    "        '''\n",
    "        predict_vector = np.zeros((len(test_data), 1))\n",
    "        for i in range(self.n_estimator):\n",
    "            predict_temp = np.array([self.model_dict[i].predict_instance(x) for x in test_data]).reshape(-1, 1)\n",
    "            predict_vector += self.learning_rate * predict_temp\n",
    "        return predict_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = x_train[:, 0].min() - 1, x_train[:, 0].max() + 1\n",
    "y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbt = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbt.fit(x_train, y_train)\n",
    "                   \n",
    "    Z = np.sign(gbt.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_train[:, 0], x_train[:, 1], c=y_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D GBM visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_krr_train = np.loadtxt('krr-train.txt')\n",
    "data_krr_test = np.loadtxt('krr-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_krr_train, y_krr_train = data_krr_train[:,0].reshape(-1,1),data_krr_train[:,1].reshape(-1,1)\n",
    "x_krr_test, y_krr_test = data_krr_test[:,0].reshape(-1,1),data_krr_test[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_size = 0.001\n",
    "x_range = np.arange(0., 1., plot_size).reshape(-1, 1)\n",
    "\n",
    "f2, axarr2 = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbm_1d = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbm_1d.fit(x_krr_train, y_krr_train)\n",
    "    \n",
    "    y_range_predict = gbm_1d.predict(x_range)\n",
    "\n",
    "    axarr2[idx[0], idx[1]].plot(x_range, y_range_predict, color='r')\n",
    "    axarr2[idx[0], idx[1]].scatter(x_krr_train, y_krr_train, alpha=0.8)\n",
    "    axarr2[idx[0], idx[1]].set_title(tt)\n",
    "    axarr2[idx[0], idx[1]].set_xlim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
